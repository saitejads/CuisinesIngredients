{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's cooking kernel !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"F:\\\\python\\\\cooking\\\\train.json\")\n",
    "testset = pd.read_json(\"F:\\\\python\\\\cooking\\\\test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>[sausage links, fennel bulb, fronds, olive oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29752</td>\n",
       "      <td>[meat cuts, file powder, smoked sausage, okra,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35687</td>\n",
       "      <td>[ground black pepper, salt, sausage casings, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...\n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...\n",
       "2  41580  [sausage links, fennel bulb, fronds, olive oil...\n",
       "3  29752  [meat cuts, file powder, smoked sausage, okra,...\n",
       "4  35687  [ground black pepper, salt, sausage casings, l..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuisine        0\n",
       "id             0\n",
       "ingredients    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "ingredients    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different types of cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greek', 'southern_us', 'filipino', 'indian', 'jamaican',\n",
       "       'spanish', 'italian', 'mexican', 'chinese', 'british', 'thai',\n",
       "       'vietnamese', 'cajun_creole', 'brazilian', 'french', 'japanese',\n",
       "       'irish', 'korean', 'moroccan', 'russian'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cuisine.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ingredients to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.astype('str')\n",
    "testset.ingredients = testset.ingredients.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['baking powder', 'eggs', 'all-purpose flour', 'raisins', 'milk', 'white sugar']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove those unnecessary symbols, which might be problem when tokenizing and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.str.replace(\"[\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"]\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"'\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  romaine lettuce    black olives    grape tomatoes    garlic    pepper    purple onion    seasoning    garbanzo beans    feta cheese crumbles  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  baking powder    eggs    all-purpose flour    raisins    milk    white sugar  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything to lower ( I think they are already in lower case, but to be on safe side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.str.lower()\n",
    "testset.ingredients = testset.ingredients.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets TOKENIZE the data now. (the processing of splitting into individual words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.apply(lambda x: word_tokenize(x))\n",
    "testset.ingredients = testset.ingredients.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets LEMMATIZE the data now (Since i believe that dataset might have different representation of same words, like the olives and olive, tomatoes and tomato, which represent the same word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmat(wor):\n",
    "    l = []\n",
    "    for i in wor:\n",
    "        l.append(lemmatizer.lemmatize(i))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.apply(lemmat)\n",
    "testset.ingredients = testset.ingredients.apply(lemmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romaine',\n",
       " 'lettuce',\n",
       " 'black',\n",
       " 'olive',\n",
       " 'grape',\n",
       " 'tomato',\n",
       " 'garlic',\n",
       " 'pepper',\n",
       " 'purple',\n",
       " 'onion',\n",
       " 'seasoning',\n",
       " 'garbanzo',\n",
       " 'bean',\n",
       " 'feta',\n",
       " 'cheese',\n",
       " 'crumbles']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baking',\n",
       " 'powder',\n",
       " 'egg',\n",
       " 'all-purpose',\n",
       " 'flour',\n",
       " 'raisin',\n",
       " 'milk',\n",
       " 'white',\n",
       " 'sugar']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that olives converted to olive, tomatoes to tomato etc, many words are now in their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.ingredients[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization converted it back to list, so change to str again and remove the unncessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ingredients = df.ingredients.astype('str')\n",
    "df.ingredients = df.ingredients.str.replace(\"[\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"]\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\"'\",\" \")\n",
    "df.ingredients = df.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.ingredients = testset.ingredients.astype('str')\n",
    "testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\n",
    "testset.ingredients = testset.ingredients.str.replace(\",\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  romaine    lettuce    black    olive    grape    tomato    garlic    pepper    purple    onion    seasoning    garbanzo    bean    feta    cheese    crumbles  '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data looks good for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect.fit_transform(df.ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39774x2826 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 756474 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now our features has 2826 features, which are created by the process of vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize some random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mora',\n",
       " 'morcilla',\n",
       " 'morel',\n",
       " 'moroccan',\n",
       " 'morsel',\n",
       " 'mortadella',\n",
       " 'morton',\n",
       " 'moss',\n",
       " 'mostaccioli',\n",
       " 'mostarda',\n",
       " 'moulard',\n",
       " 'mountain',\n",
       " 'mousse',\n",
       " 'mozarella',\n",
       " 'mozzarella',\n",
       " 'mrs',\n",
       " 'msg',\n",
       " 'muenster',\n",
       " 'muesli',\n",
       " 'muffin']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[1650:1670]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets vectorize our testset as well, we only tranform it with already fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeatures = vect.transform(testset.ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9944x2826 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 189531 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our labels now, which is obviously cuisine column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the dataset into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes, to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 2826) (7955, 2826) (31819,) (7955,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=12, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=12)\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy 0.7732243871778756\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression accuracy\",logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mexican', 'thai', 'mexican', ..., 'indian', 'indian', 'japanese'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP.DESKTOP-UCJPQBV\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "sgd = linear_model.SGDClassifier()\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD classifier accuracy 0.7417976115650534\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD classifier accuracy\",sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1500,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linearsvm = LinearSVC(random_state=0, max_iter = 1500)\n",
    "linearsvm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy 0.7713387806411062\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVM accuracy\", linearsvm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 77.4 with the logistic regression i s the best we acheived so far with out any tuning of parameters.\n",
    "Now, lets try our luck with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK'S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried both Keras and tensorflow (Of course the backend is same), but Keras code looks simpler and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Neural Networks we need to have the dense array's as inputs and preferably one hot encoding for lables.\n",
    "So, lets create lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = df.cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to one hot formatting, there are many ways to do, i prefer to do this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = pd.get_dummies(labelsNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to arrays, you can do by values method or np.array() both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNN = labelsNN.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the one hot encoding looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsNN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are ready, now we need the features, we have already created the features above but it was sparse matrix, which neural network doesnt like, so convert to dense arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(features)\n",
    "featuresNN = sparse_dataset.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the features look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresNN[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainNN, X_testNN, y_trainNN, y_testNN = train_test_split(featuresNN, labelsNN, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31819, 2826) (7955, 2826) (31819, 20) (7955, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_trainNN.shape, X_testNN.shape, y_trainNN.shape, y_testNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential NN with 300,500 and 400 nodes in first,second and third layers resp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is categorical cross entropy and the optimizer is adam with default learning rate.\n",
    "We can tweak a lot of parameters like the no of nodes, epochs, batchsize etc to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 1.5324 - categorical_accuracy: 0.5639\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.7531 - categorical_accuracy: 0.7771\n",
      "Epoch 3/50\n",
      " - 3s - loss: 0.5700 - categorical_accuracy: 0.8309\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.4529 - categorical_accuracy: 0.8637\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.3596 - categorical_accuracy: 0.8933\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.2790 - categorical_accuracy: 0.9158\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.2128 - categorical_accuracy: 0.9384\n",
      "Epoch 8/50\n",
      " - 4s - loss: 0.1644 - categorical_accuracy: 0.9520\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.1191 - categorical_accuracy: 0.9680\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.0884 - categorical_accuracy: 0.9759\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.0626 - categorical_accuracy: 0.9849\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.0459 - categorical_accuracy: 0.9893\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.0354 - categorical_accuracy: 0.9924\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.0264 - categorical_accuracy: 0.9948\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.0228 - categorical_accuracy: 0.9954\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.0225 - categorical_accuracy: 0.9953\n",
      "Epoch 17/50\n",
      " - 4s - loss: 0.0184 - categorical_accuracy: 0.9965\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.0230 - categorical_accuracy: 0.9947\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.0327 - categorical_accuracy: 0.9911\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.0348 - categorical_accuracy: 0.9894\n",
      "Epoch 21/50\n",
      " - 3s - loss: 0.0397 - categorical_accuracy: 0.9881\n",
      "Epoch 22/50\n",
      " - 3s - loss: 0.0344 - categorical_accuracy: 0.9900\n",
      "Epoch 23/50\n",
      " - 3s - loss: 0.0280 - categorical_accuracy: 0.9920\n",
      "Epoch 24/50\n",
      " - 3s - loss: 0.0243 - categorical_accuracy: 0.9937\n",
      "Epoch 25/50\n",
      " - 3s - loss: 0.0165 - categorical_accuracy: 0.9961\n",
      "Epoch 26/50\n",
      " - 3s - loss: 0.0112 - categorical_accuracy: 0.9978\n",
      "Epoch 27/50\n",
      " - 3s - loss: 0.0092 - categorical_accuracy: 0.9985\n",
      "Epoch 28/50\n",
      " - 4s - loss: 0.0095 - categorical_accuracy: 0.9981\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.0098 - categorical_accuracy: 0.9979\n",
      "Epoch 30/50\n",
      " - 3s - loss: 0.0084 - categorical_accuracy: 0.9983\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.0107 - categorical_accuracy: 0.9978\n",
      "Epoch 32/50\n",
      " - 3s - loss: 0.0122 - categorical_accuracy: 0.9972\n",
      "Epoch 33/50\n",
      " - 4s - loss: 0.0121 - categorical_accuracy: 0.9975\n",
      "Epoch 34/50\n",
      " - 4s - loss: 0.0183 - categorical_accuracy: 0.9954\n",
      "Epoch 35/50\n",
      " - 4s - loss: 0.0205 - categorical_accuracy: 0.9938\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.0285 - categorical_accuracy: 0.9910\n",
      "Epoch 37/50\n",
      " - 4s - loss: 0.0441 - categorical_accuracy: 0.9859\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.0408 - categorical_accuracy: 0.9871\n",
      "Epoch 39/50\n",
      " - 4s - loss: 0.0224 - categorical_accuracy: 0.9941\n",
      "Epoch 40/50\n",
      " - 4s - loss: 0.0119 - categorical_accuracy: 0.9975\n",
      "Epoch 41/50\n",
      " - 4s - loss: 0.0096 - categorical_accuracy: 0.9979\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.0102 - categorical_accuracy: 0.9975\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.0092 - categorical_accuracy: 0.9983\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.0064 - categorical_accuracy: 0.9989\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.0055 - categorical_accuracy: 0.9993\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.0055 - categorical_accuracy: 0.9992\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.0052 - categorical_accuracy: 0.9992\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.0052 - categorical_accuracy: 0.9991\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.0062 - categorical_accuracy: 0.9989\n",
      "Epoch 50/50\n",
      " - 3s - loss: 0.0058 - categorical_accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20247f24160>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Dense(300,input_dim = 2826,activation = 'relu'))\n",
    "model.add(Dense(500,activation = 'relu'))\n",
    "model.add(Dense(400,activation = 'relu'))\n",
    "model.add(Dense(20,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\n",
    "model.fit(X_trainNN,y_trainNN,epochs=50,shuffle=True, verbose =2,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7955/7955 [==============================] - 1s 88us/step\n",
      "Accuracy with KERAS 0.7864236329502463\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy with KERAS\" ,model.evaluate(X_testNN,y_testNN)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained with KERAS on my pc for few times and achieved max accuracy of 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try with tensorflow (technically it same as before, but i just want to show you how it works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None,2826]) # Since we have 2826 features. \n",
    "y = tf.placeholder(tf.float32,[None,20])  # Since we have 20 outut labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = tf.get_variable(\"weights1\",shape=[2826,600],initializer = tf.contrib.layers.xavier_initializer())\n",
    "biases1 = tf.get_variable(\"biases1\",shape = [600],initializer = tf.zeros_initializer)\n",
    "layer1out = tf.nn.relu(tf.matmul(X,weights1)+biases1)\n",
    "\n",
    "weights2 = tf.get_variable(\"weights2\",shape=[600,900],initializer = tf.contrib.layers.xavier_initializer())\n",
    "biases2 = tf.get_variable(\"biases2\",shape = [900],initializer = tf.zeros_initializer)\n",
    "layer2out = tf.nn.relu(tf.matmul(layer1out,weights2)+biases2)\n",
    "\n",
    "weights3 = tf.get_variable(\"weights3\",shape=[900,20],initializer = tf.contrib.layers.xavier_initializer())\n",
    "biases3 = tf.get_variable(\"biases3\",shape = [20],initializer = tf.zeros_initializer)\n",
    "prediction =tf.matmul(layer2out,weights3)+biases3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is NN with 600,900 nodes in first and second layer resp.\n",
    "Xavier initializer is usually advised over random weights and Zeros initializer over random biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- Cost 96229.016\n",
      "Epoch 10 -- Cost 56917.8\n",
      "Epoch 20 -- Cost 35608.418\n",
      "Epoch 30 -- Cost 25354.799\n",
      "Epoch 40 -- Cost 19556.654\n",
      "Epoch 50 -- Cost 15888.187\n",
      "Epoch 60 -- Cost 13356.782\n",
      "Epoch 70 -- Cost 11364.489\n",
      "Epoch 80 -- Cost 9657.299\n",
      "Epoch 90 -- Cost 8130.496\n",
      "Epoch 100 -- Cost 6761.1396\n",
      "Accuracy on the test set -> 0.7791326\n",
      "FINISHED !!!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(101): # no of epochs\n",
    "        opt,costval = sess.run([optimizer,cost],feed_dict = {X:X_trainNN,y:y_trainNN})\n",
    "        matches = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(matches, 'float'))\n",
    "        if (epoch % 10 == 0):\n",
    "            print(\"Epoch\", epoch, \"--\" , \"Cost\",costval)\n",
    "    print(\"Accuracy on the test set ->\",accuracy.eval({X:X_testNN,y:y_testNN}))\n",
    "    print(\"FINISHED !!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have achieved almost similar accuracies in all the above models, I personally dont prefer NN's on this data as it is computationally very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer just using the logisticRegression for predictions, but linearSVC also has almost same results.\n",
    "I'm not predict using Keras or Tensorflow, since it needs an extra two steps to convert the labels, which I dont want to waste my time on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg.predict(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id':testset.id,'cuisine':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sub[['id','cuisine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "1) You can achieve better accuracy by tuning the parameters.\n",
    "2) Neural Network has even scored an accuracy of 0.81 but the computation is very time taking.\n",
    "3) I have not used my time on visualizing the dataset.(which is not needed for this submission.\n",
    "4) Please comment for any questions, doubts or suggestions.\n",
    "\n",
    " THANK YOU\n",
    " \n",
    "# please UPVOTE, if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
